{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, RandomSampler\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "from datasets import load_dataset, DatasetDict, Dataset, ClassLabel\n",
    "from copy import deepcopy\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "random_seed = 17283"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset glue (/home/lestienne/.cache/huggingface/datasets/glue/qqp/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.0075380802154541016,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 3,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2987f965ca6748e2a0af5aa5fbe601c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached split indices for dataset at /home/lestienne/.cache/huggingface/datasets/glue/qqp/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-087f7b863a619629.arrow and /home/lestienne/.cache/huggingface/datasets/glue/qqp/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-bcfd9d7760ac776b.arrow\n",
      "Loading cached split indices for dataset at /home/lestienne/.cache/huggingface/datasets/glue/qqp/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-66c4bebe9ed69451.arrow and /home/lestienne/.cache/huggingface/datasets/glue/qqp/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad/cache-1f16b6c43125cf85.arrow\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.0076141357421875,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 569,
       "unit": "ba",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "554846cf1f0748cebbd43e4059237bd2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/569 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.00767064094543457,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 143,
       "unit": "ba",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85775a0972f64a5b949ccbe3cea7b429",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/143 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.0076525211334228516,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 764,
       "unit": "ba",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee090dd078224eb3a641c7b36a67b68a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/764 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def download_qqd_data(random_seed):\n",
    "\n",
    "    qqp_data = load_dataset(\"glue\",\"qqp\")\n",
    "    qqp_train_splitted = qqp_data[\"train\"].train_test_split(test_size=0.2,seed=random_seed,stratify_by_column=\"label\")\n",
    "    qqp_val_splitted = qqp_data[\"validation\"].train_test_split(test_size=0.2,seed=random_seed,stratify_by_column=\"label\")\n",
    "\n",
    "    data = {\n",
    "        \"training\": DatasetDict({\n",
    "            \"train\": qqp_train_splitted[\"train\"],\n",
    "            \"validation\": qqp_train_splitted[\"test\"],\n",
    "            \"test\": qqp_data[\"test\"]\n",
    "        }),\n",
    "        \"calibration\": DatasetDict({\n",
    "            \"train\": qqp_val_splitted[\"train\"],\n",
    "            \"validation\": qqp_val_splitted[\"test\"],\n",
    "            \"test\": deepcopy(qqp_data[\"test\"])\n",
    "        })\n",
    "    }\n",
    "    return data\n",
    "\n",
    "def create_tokenizer():\n",
    "    return AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "def create_dataloaders(datasets,tokenizer,random_seed=0,batch_size=32):\n",
    "\n",
    "    def apply_encoding(tokenizer,sample):\n",
    "        return tokenizer(\n",
    "            text=sample[\"question1\"],\n",
    "            text_pair=sample[\"question2\"],\n",
    "            padding=\"do_not_pad\",\n",
    "            truncation=True,\n",
    "            max_length=256,\n",
    "        )\n",
    "\n",
    "    def data_collator(tokenizer,features):\n",
    "        return tokenizer.pad(\n",
    "            features,\n",
    "            padding=\"longest\",\n",
    "            max_length=None,\n",
    "            pad_to_multiple_of=None,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "\n",
    "    generator = torch.Generator()\n",
    "    generator.manual_seed(int(random_seed))\n",
    "\n",
    "    dataloaders = {}\n",
    "    for name, dataset in datasets.items():\n",
    "        if not isinstance(dataset.features[\"label\"],ClassLabel):\n",
    "            dataset = dataset.class_encode_column(\"label\")\n",
    "        dataset = dataset.map(\n",
    "            lambda sample: apply_encoding(tokenizer,sample),\n",
    "            batched=True,\n",
    "            batch_size=512,\n",
    "            remove_columns=[column for column in dataset.column_names if column not in [\"input_ids\",\"attention_mask\",\"token_type_ids\",\"label\"]]\n",
    "        )\n",
    "        dataloaders[name] = DataLoader(\n",
    "            dataset,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=False,\n",
    "            sampler=RandomSampler(dataset,num_samples=len(dataset),generator=generator),\n",
    "            num_workers=8,\n",
    "            collate_fn=lambda features: data_collator(tokenizer,features)\n",
    "        )\n",
    "\n",
    "    return dataloaders\n",
    "\n",
    "\n",
    "data = download_qqd_data(random_seed)\n",
    "tokenizer = create_tokenizer()\n",
    "dataloaders = create_dataloaders(data[\"training\"],tokenizer,random_seed=0,batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CBOW(\n",
       "  (linear_input): Embedding(30522, 400, padding_idx=0)\n",
       "  (linear_output): Linear(in_features=400, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import torch.nn as nn\n",
    "\n",
    "class CBOW(nn.Module):\n",
    "\n",
    "    def __init__(self,tokenizer,hidden_size,output_size):\n",
    "        super().__init__()\n",
    "        num_embeddings = len(tokenizer)\n",
    "        self.linear_input = nn.Embedding(\n",
    "            num_embeddings=num_embeddings,\n",
    "            embedding_dim=hidden_size,\n",
    "            padding_idx=tokenizer.pad_token_id\n",
    "        )\n",
    "        self.linear_output = nn.Linear(in_features=hidden_size,out_features=output_size)\n",
    "\n",
    "    def forward(self,input_ids,token_type_ids,attention_mask):\n",
    "        x = self.linear_input(input_ids).mean(dim=1)\n",
    "        x = torch.relu(x)\n",
    "        logits = self.linear_output(x)\n",
    "        return {\"logits\": logits}\n",
    "\n",
    "\n",
    "\n",
    "model_name = \"cbow\"\n",
    "if model_name == \"cbow\":\n",
    "    model = CBOW(tokenizer,hidden_size=400,output_size=1)\n",
    "elif model_name == \"bert\":\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-uncased\",num_labels=1)\n",
    "results_dir = f\"../results/{model_name}/run00_0/07_Train model/version_0/checkpoints\"\n",
    "state_dict = torch.load(os.path.join(results_dir,os.listdir(results_dir)[0]))[\"state_dict\"]\n",
    "model.load_state_dict({\n",
    "    key.split(\"model.\")[-1]: state_dict[f\"{key}\"] for key in state_dict.keys()\n",
    "})\n",
    "model\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/9097 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "  0%|          | 1/9097 [00:00<48:27,  3.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████▉| 9084/9097 [00:11<00:00, 833.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9097/9097 [00:11<00:00, 802.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2275 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|█████████▋| 2208/2275 [00:03<00:00, 837.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2275/2275 [00:03<00:00, 699.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/12218 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "100%|█████████▉| 12191/12218 [00:14<00:00, 811.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12218/12218 [00:15<00:00, 811.85it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "results = {}\n",
    "for split, dataloader in dataloaders.items():\n",
    "    all_labels = []\n",
    "    all_logits = []\n",
    "    print(split)\n",
    "    for batch in tqdm(dataloader):\n",
    "        labels = batch.pop(\"label\").view(-1)\n",
    "        all_labels.append(labels)\n",
    "        with torch.no_grad():\n",
    "            logits = model(**batch)[\"logits\"].view(-1)\n",
    "            all_logits.append(logits)\n",
    "    all_labels = torch.cat(all_labels).numpy()\n",
    "    all_logits = torch.cat(all_logits).numpy()\n",
    "    results[split] = {\n",
    "        \"labels\": all_labels,\n",
    "        \"logits\": all_logits\n",
    "    }\n",
    "\n",
    "import pickle\n",
    "with open(f\"../{model_name}_results.pkl\",\"wb\") as f:\n",
    "    pickle.dump(results,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': {'labels': array([1, 1, 0, ..., 0, 1, 0]),\n",
       "  'logits': array([ -0.47506553,  -1.0199497 ,  -1.951016  , ..., -14.371104  ,\n",
       "           0.94091755,  -5.0620985 ], dtype=float32)},\n",
       " 'validation': {'labels': array([0, 0, 0, ..., 1, 0, 0]),\n",
       "  'logits': array([-5.366811 , -2.6259027, -1.871501 , ..., -1.1364172, -5.1908154,\n",
       "         -2.0357196], dtype=float32)},\n",
       " 'test': {'labels': array([-1, -1, -1, ..., -1, -1, -1]),\n",
       "  'logits': array([-0.9306036 , -0.04512712,  1.8851213 , ..., -3.499448  ,\n",
       "         -2.3883498 , -4.3523874 ], dtype=float32)}}"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8662635701525354"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy = np.mean(all_labels == all_predictions)\n",
    "accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 900x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(1,1,figsize=(9,4))\n",
    "ax.hist(torch.sigmoid(logits.view(-1)),bins=10);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 ('nlp')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f90631fd6269ecec6b41fd2152e8b981085596aa19ac7b6fcd4ee9a0c7858ce4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
